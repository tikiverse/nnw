#!/usr/bin/env python3
"""
Query NetNewsWire data for LLM summarization.

Basic Usage:
    nnw                    # All unread articles
    nnw -t                 # Today's unread articles (last 1 day)
    nnw --all              # All articles (including read)
    nnw --starred          # Starred articles only
    nnw --days 3           # Articles from last 3 days
    nnw --feeds            # List all feeds
    nnw --feed "name"       # Articles from feed matching name (fuzzy)
    nnw --feed "url"        # Articles from specific feed (exact URL)
    nnw -ff "pattern"      # Articles from all feeds matching pattern
    nnw -a URL             # Lookup a single article by URL
    nnw --limit 50         # Limit number of articles
    nnw --format json      # Output as JSON instead of markdown
    nnw --full             # Include full article content (no truncation)

LLM Integration:
    # Generate a prompt for summarization
    nnw --days 1 --format markdown --prompt "Summarize these articles"

    # Pipe directly to Claude CLI
    nnw --days 1 --format markdown --prompt "Summarize each article in 2 sentences" | claude

    # Copy to clipboard for pasting into a chat interface
    nnw --days 1 --format markdown --prompt "What are the key themes?" | pbcopy
"""

import argparse
import html
import json
import os
import plistlib
import re
import sqlite3
import sys
import xml.etree.ElementTree as ET
from datetime import datetime, timedelta
from pathlib import Path


# NetNewsWire data paths
NNW_CONTAINER = Path.home() / "Library/Containers/com.ranchero.NetNewsWire-Evergreen/Data/Library/Application Support/NetNewsWire/Accounts/OnMyMac"
DB_PATH = NNW_CONTAINER / "DB.sqlite3"
FEED_METADATA_PATH = NNW_CONTAINER / "FeedMetadata.plist"
SUBSCRIPTIONS_PATH = NNW_CONTAINER / "Subscriptions.opml"


def terminal_link(url: str, text: str) -> str:
    """Wrap text in OSC 8 hyperlink escape sequence."""
    return f'\033]8;;{url}\033\\{text}\033]8;;\033\\'


def strip_html(text: str) -> str:
    """Remove HTML tags and decode entities."""
    if not text:
        return ''
    # Remove HTML tags
    clean = re.sub(r'<[^>]+>', ' ', text)
    # Decode HTML entities
    clean = html.unescape(clean)
    # Normalize whitespace
    clean = re.sub(r'\s+', ' ', clean).strip()
    return clean


def load_feed_metadata() -> dict:
    """Load feed metadata from plist file."""
    if not FEED_METADATA_PATH.exists():
        return {}

    with open(FEED_METADATA_PATH, 'rb') as f:
        return plistlib.load(f)


def load_feed_names_from_opml() -> dict:
    """Load feed names from OPML subscriptions file.

    Returns:
        Dict mapping feed URL (xmlUrl) to feed title
    """
    if not SUBSCRIPTIONS_PATH.exists():
        return {}

    feed_names = {}
    try:
        tree = ET.parse(SUBSCRIPTIONS_PATH)
        root = tree.getroot()

        # Find all outline elements with xmlUrl (these are feeds)
        for outline in root.iter('outline'):
            xml_url = outline.get('xmlUrl')
            if xml_url:
                # Prefer title, fall back to text
                title = outline.get('title') or outline.get('text')
                if title:
                    feed_names[xml_url] = title
    except ET.ParseError:
        pass

    return feed_names


def get_feed_name(feed_id: str, metadata: dict, opml_names: dict = None) -> str:
    """Get human-readable feed name from metadata.

    Priority: editedName (user renamed) > OPML title > feed ID
    """
    feed_info = metadata.get(feed_id, {})
    # Prefer edited name (user renamed in NNW UI)
    if feed_info.get('editedName'):
        return feed_info['editedName']
    # Then OPML title (original feed title)
    if opml_names and feed_id in opml_names:
        return opml_names[feed_id]
    # Fall back to feed ID (URL)
    return feed_id


def query_feeds(metadata: dict, opml_names: dict = None) -> list:
    """Get list of all feeds with metadata."""
    feeds = []
    for feed_id, info in metadata.items():
        feeds.append({
            'feed_url': feed_id,
            'name': get_feed_name(feed_id, metadata, opml_names),
            'home_page': info.get('homePageURL'),
            'last_check': str(info.get('lastCheckDate', ''))
        })
    return sorted(feeds, key=lambda x: x['name'].lower())


def resolve_feed_filter(search_term: str, metadata: dict, opml_names: dict = None) -> str:
    """Resolve a --feed argument to a feed URL.

    Args:
        search_term: Either an exact feed URL or a substring to match against feed names
        metadata: Feed metadata dictionary
        opml_names: Dict mapping feed URL to title from OPML

    Returns:
        The matching feed URL

    Raises:
        SystemExit: If no match found or multiple matches (ambiguous)
    """
    # First, check for exact URL match
    if search_term in metadata:
        return search_term

    # Otherwise, do case-insensitive substring match on feed names
    matches = []
    search_lower = search_term.lower()

    for feed_url, info in metadata.items():
        name = get_feed_name(feed_url, metadata, opml_names)
        if search_lower in name.lower():
            matches.append({'name': name, 'feed_url': feed_url})

    if len(matches) == 0:
        # No matches - show available feeds
        feeds = query_feeds(metadata, opml_names)
        feed_names = [f"  {f['name']}" for f in feeds]
        print(f'No feed matched: "{search_term}"', file=sys.stderr)
        print("Available feeds:", file=sys.stderr)
        print('\n'.join(feed_names), file=sys.stderr)
        sys.exit(1)

    if len(matches) == 1:
        return matches[0]['feed_url']

    # Multiple matches - ambiguous
    print(f'Ambiguous feed: "{search_term}"', file=sys.stderr)
    print("Multiple feeds matched:", file=sys.stderr)
    for m in sorted(matches, key=lambda x: x['name'].lower()):
        print(f"  {m['name']}: {m['feed_url']}", file=sys.stderr)
    sys.exit(1)


def resolve_feed_filter_pattern(pattern: str, metadata: dict, opml_names: dict = None) -> list:
    """Resolve a --feed-filter pattern to multiple feed URLs.

    Unlike resolve_feed_filter, this allows ambiguous matches and returns all matching feeds.

    Args:
        pattern: A substring to match against feed names or URLs
        metadata: Feed metadata dictionary
        opml_names: Dict mapping feed URL to title from OPML

    Returns:
        List of matching feed URLs

    Raises:
        SystemExit: If no match found
    """
    matches = []
    pattern_lower = pattern.lower()

    for feed_url, info in metadata.items():
        name = get_feed_name(feed_url, metadata, opml_names)
        # Match against feed name or feed URL
        if pattern_lower in name.lower() or pattern_lower in feed_url.lower():
            matches.append(feed_url)

    if len(matches) == 0:
        feeds = query_feeds(metadata, opml_names)
        feed_names = [f"  {f['name']}" for f in feeds]
        print(f'No feeds matched pattern: "{pattern}"', file=sys.stderr)
        print("Available feeds:", file=sys.stderr)
        print('\n'.join(feed_names), file=sys.stderr)
        sys.exit(1)

    return matches


def query_articles(
    conn: sqlite3.Connection,
    metadata: dict,
    unread_only: bool = True,
    starred_only: bool = False,
    days: int = None,
    feed_urls: list = None,
    limit: int = 100,
    full_content: bool = False,
    oldest_first: bool = False,
    opml_names: dict = None
) -> list:
    """Query articles from the database."""

    # Build query
    query = """
        SELECT
            a.articleID,
            a.feedID,
            a.title,
            a.url,
            a.externalURL,
            a.summary,
            a.contentText,
            a.contentHTML,
            a.datePublished,
            s.read,
            s.starred,
            s.dateArrived
        FROM articles a
        LEFT JOIN statuses s ON a.articleID = s.articleID
        WHERE 1=1
    """
    params = []

    # Add date filter if days specified
    if days is not None:
        threshold = datetime.now() - timedelta(days=days)
        threshold_ts = threshold.timestamp()
        query += " AND a.datePublished > ?"
        params.append(threshold_ts)

    if unread_only and not starred_only:
        query += " AND (s.read IS NULL OR s.read = 0)"

    if starred_only:
        query += " AND s.starred = 1"

    if feed_urls:
        placeholders = ', '.join('?' * len(feed_urls))
        query += f" AND a.feedID IN ({placeholders})"
        params.extend(feed_urls)

    order = "ASC" if oldest_first else "DESC"
    query += f" ORDER BY a.datePublished {order}"

    if limit:
        query += f" LIMIT {limit}"

    cursor = conn.execute(query, params)

    articles = []
    for row in cursor.fetchall():
        article_id, feed_id, title, url, external_url, summary, content_text, content_html, date_published, read, starred, date_arrived = row

        # Convert timestamp to readable date
        if date_published:
            try:
                pub_date = datetime.fromtimestamp(date_published).strftime('%Y-%m-%d %H:%M')
            except:
                pub_date = str(date_published)
        else:
            pub_date = None

        # Get content - prefer text, then strip HTML
        content = content_text or strip_html(content_html) or summary or ''
        if full_content:
            content_preview = content
        else:
            content_preview = content[:500] + '...' if len(content) > 500 else content

        articles.append({
            'feed': get_feed_name(feed_id, metadata, opml_names),
            'feed_url': feed_id,
            'title': title or '(no title)',
            'url': url or external_url,
            'published': pub_date,
            'read': bool(read),
            'starred': bool(starred),
            'summary': content_preview
        })

    return articles


def query_article_by_url(
    conn: sqlite3.Connection,
    metadata: dict,
    article_url: str,
    full_content: bool = False,
    opml_names: dict = None
) -> dict | None:
    """Query a single article by its URL (checks both url and externalURL columns)."""
    query = """
        SELECT
            a.articleID,
            a.feedID,
            a.title,
            a.url,
            a.externalURL,
            a.summary,
            a.contentText,
            a.contentHTML,
            a.datePublished,
            s.read,
            s.starred,
            s.dateArrived
        FROM articles a
        LEFT JOIN statuses s ON a.articleID = s.articleID
        WHERE a.url = ? OR a.externalURL = ?
    """
    cursor = conn.execute(query, [article_url, article_url])
    row = cursor.fetchone()

    if not row:
        return None

    article_id, feed_id, title, url, external_url, summary, content_text, content_html, date_published, read, starred, date_arrived = row

    # Convert timestamp to readable date
    if date_published:
        try:
            pub_date = datetime.fromtimestamp(date_published).strftime('%Y-%m-%d %H:%M')
        except:
            pub_date = str(date_published)
    else:
        pub_date = None

    # Get content - prefer text, then strip HTML
    content = content_text or strip_html(content_html) or summary or ''
    if full_content:
        content_preview = content
    else:
        content_preview = content[:500] + '...' if len(content) > 500 else content

    return {
        'feed': get_feed_name(feed_id, metadata, opml_names),
        'feed_url': feed_id,
        'title': title or '(no title)',
        'url': url or external_url,
        'published': pub_date,
        'read': bool(read),
        'starred': bool(starred),
        'summary': content_preview
    }


def format_as_markdown(data: dict, use_links: bool = False) -> str:
    """Format output as markdown for easier reading."""
    lines = []

    if 'feeds' in data:
        lines.append("# NetNewsWire Feeds\n")
        for feed in data['feeds']:
            lines.append(f"- **{feed['name']}**")
            feed_url = feed['feed_url']
            if use_links:
                feed_url = terminal_link(feed_url, feed_url)
            lines.append(f"  - URL: {feed_url}")
            if feed['home_page']:
                home_page = feed['home_page']
                if use_links:
                    home_page = terminal_link(home_page, home_page)
                lines.append(f"  - Home: {home_page}")
            lines.append("")

    if 'articles' in data:
        lines.append(f"# Articles ({data['count']} total)\n")
        for article in data['articles']:
            starred = "â­ " if article['starred'] else ""
            read_status = "" if article['read'] else "ðŸ“¬ "
            lines.append(f"## {starred}{read_status}{article['title']}")
            lines.append(f"**Feed:** {article['feed']}")
            lines.append(f"**Published:** {article['published']}")
            if article['url']:
                url = article['url']
                if use_links:
                    url = terminal_link(url, url)
                lines.append(f"**Link:** {url}")
            if article['summary']:
                lines.append(f"\n{article['summary']}")
            lines.append("\n---\n")

    return '\n'.join(lines)


def format_as_summary(data: dict, use_links: bool = False) -> str:
    """Format output as condensed summary grouped by feed."""
    lines = []

    if 'feeds' in data:
        # For feeds listing, just show names
        lines.append(f"Feeds ({data['count']})")
        for feed in data['feeds']:
            lines.append(f"- {feed['name']}")
        return '\n'.join(lines)

    if 'articles' in data:
        # Total count header
        lines.append(f"{data['count']} articles")
        lines.append("")

        # Group articles by feed
        feeds = {}
        for article in data['articles']:
            feed_name = article['feed']
            if feed_name not in feeds:
                feeds[feed_name] = []
            feeds[feed_name].append({'title': article['title'], 'url': article.get('url')})

        # Output grouped by feed
        for feed_name in sorted(feeds.keys(), key=str.lower):
            articles = feeds[feed_name]
            lines.append(f"{feed_name} ({len(articles)})")
            for article in articles:
                # Normalize whitespace and truncate for summary view
                title = re.sub(r'\s+', ' ', article['title']).strip()
                if len(title) > 80:
                    title = title[:79] + 'â€¦'
                if use_links and article['url']:
                    title = terminal_link(article['url'], title)
                lines.append(f"- {title}")
            lines.append("")

    return '\n'.join(lines).rstrip()


def main():
    parser = argparse.ArgumentParser(description='Query NetNewsWire data for LLM summarization')
    parser.add_argument('-a', '--article', type=str, metavar='URL', help='Lookup a single article by URL')
    parser.add_argument('--feeds', action='store_true', help='List all feeds')
    parser.add_argument('--all', action='store_true', help='Include read articles')
    parser.add_argument('--starred', action='store_true', help='Only starred articles')
    parser.add_argument('-d', '--days', type=int, default=None, help='Number of days to look back (default: all)')
    parser.add_argument('-t', '--today', action='store_true', help='Articles from today only (shorthand for -d 1)')
    parser.add_argument('-f', '--feed', type=str, action='append', metavar='NAME_OR_URL',
                        help='Filter by feed (fuzzy name match or exact URL). Can be used multiple times.')
    parser.add_argument('-ff', '--feed-filter', type=str, action='append', metavar='PATTERN',
                        help='Filter by feeds matching pattern (allows multiple matches). Can be used multiple times.')
    parser.add_argument('--limit', type=int, default=100, help='Max articles to return (default: 100)')
    parser.add_argument('-s', action='store_true', help='Condensed summary format (same as --format summary)')
    parser.add_argument('--format', choices=['json', 'markdown', 'md', 'summary'], default='markdown', help='Output format (default: markdown)')
    parser.add_argument('--full', action='store_true', help='Include full article content (no truncation)')
    parser.add_argument('--truncate', action='store_true', help='Use truncated content (--prompt auto-enables --full; this reverts to 500 char limit)')
    parser.add_argument('--oldest-first', action='store_true', help='Sort oldest articles first (default: newest first)')
    parser.add_argument('--prompt', type=str, metavar='INSTRUCTION', help='Wrap output with LLM prompt (e.g., "Summarize these articles")')
    parser.add_argument('-l', '--links', action='store_true', help='Enable clickable terminal hyperlinks (OSC 8)')

    args = parser.parse_args()

    # Handle -t/--today shorthand
    if args.today:
        args.days = 1

    # Enable links if flag set or NNW_LINKS=1 environment variable
    use_links = args.links or os.environ.get('NNW_LINKS') == '1'

    # Determine full_content mode:
    # --full explicitly enables, --prompt implicitly enables (unless --truncate overrides)
    full_content = args.full or (args.prompt and not args.truncate)

    # Check if database exists
    if not DB_PATH.exists():
        print(json.dumps({'error': f'NetNewsWire database not found at {DB_PATH}'}))
        return 1

    # Load feed metadata and OPML names
    metadata = load_feed_metadata()
    opml_names = load_feed_names_from_opml()

    if args.feeds:
        # List feeds mode
        feeds = query_feeds(metadata, opml_names)
        output = {'feeds': feeds, 'count': len(feeds)}
    elif args.article:
        # Single article lookup mode
        conn = sqlite3.connect(DB_PATH)
        try:
            article = query_article_by_url(conn, metadata, args.article, full_content=full_content, opml_names=opml_names)
            if not article:
                print(f'Article not found: {args.article}', file=sys.stderr)
                return 1
            output = {'articles': [article], 'count': 1}
        finally:
            conn.close()
    else:
        # Resolve feed filters to URLs
        resolved_feeds = None
        feed_info = None
        if args.feed or args.feed_filter:
            resolved_feeds = []
            feed_info = []
            # Handle exact/unique matches from -f/--feed
            if args.feed:
                for search_term in args.feed:
                    feed_url = resolve_feed_filter(search_term, metadata, opml_names)
                    if feed_url not in resolved_feeds:
                        resolved_feeds.append(feed_url)
                        feed_name = get_feed_name(feed_url, metadata, opml_names)
                        feed_info.append({'name': feed_name, 'url': feed_url})
            # Handle pattern matches from -ff/--feed-filter
            if args.feed_filter:
                for pattern in args.feed_filter:
                    matching_urls = resolve_feed_filter_pattern(pattern, metadata, opml_names)
                    for feed_url in matching_urls:
                        if feed_url not in resolved_feeds:
                            resolved_feeds.append(feed_url)
                            feed_name = get_feed_name(feed_url, metadata, opml_names)
                            feed_info.append({'name': feed_name, 'url': feed_url})

        # Query articles mode
        conn = sqlite3.connect(DB_PATH)
        try:
            articles = query_articles(
                conn,
                metadata,
                unread_only=not args.all,
                starred_only=args.starred,
                days=args.days,
                feed_urls=resolved_feeds,
                limit=args.limit,
                full_content=full_content,
                oldest_first=args.oldest_first,
                opml_names=opml_names
            )
            output = {
                'articles': articles,
                'count': len(articles),
                'query': {
                    'unread_only': not args.all,
                    'starred_only': args.starred,
                    'days': args.days,
                    'feeds': feed_info
                }
            }
        finally:
            conn.close()

    # Handle -s shorthand for --format summary
    if args.s:
        args.format = 'summary'

    # Format output
    if args.format in ('markdown', 'md'):
        formatted = format_as_markdown(output, use_links=use_links)
    elif args.format == 'summary':
        formatted = format_as_summary(output, use_links=use_links)
    else:
        formatted = json.dumps(output, indent=2, ensure_ascii=False)

    # Wrap with prompt if requested
    if args.prompt:
        print(f"{args.prompt}\n\n<articles>\n{formatted}\n</articles>")
    else:
        print(formatted)

    return 0


if __name__ == '__main__':
    exit(main())
